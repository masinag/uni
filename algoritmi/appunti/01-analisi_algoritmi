* Analisi algoritmi

Complessità: funzione da Dimensione input -> Tempo (numero di istruzioni elementari)

Criteri di valutazione:
- costo uniforme: dim input è il numero di elementi di cui è costituito es. dimensione array
- costo logaritmico: dim numero di bit necessari per rappresentarlo es. Moltiplicazione numeri

Funzione di costo: f: N -> R

Notazioni:
* O(g(n)) è l'insieme delle funzioni f(n) tali che Esiste c>0, m>=0 tali che
    f(n) <= c*g(n) per ogni n >= m [f cresce al più come g]
* Ω(g(n)) è l'insieme delle funzioni f(n) tali che Esiste c>0, m>=0 tali che
    f(n) >= c*g(n) per ogni n >= m [f cresce almeno come g]
* ϴ(g(n)) è l'insieme  delle funzioni f(n) tali che Esiste c1>0, c2>9, m>=0 tali che
    c1*g(n) <= f(n) <= c2*g(n) per ogni n >= m [f cresce esattamente come g]
    => f = ϴ(g(n)) e f = Ω(g(n))

Complessità algoritmi vs Complessità problemi
Es. Moltiplicazione numeri complessi di Gauss
    (a + bi) * (c+di) = (ac - bd) + (ad + bc)i
    => calcolo x = (a+b) * (c+d) = ac + ad + bc + bd,
               y = ac, z = bd => 3 moltiplicazioni
    => risultato: (y - z) + (x-y-z)i
    => è possibile effettuare un numero di moltiplicazioni minore
Es. Somma numeri binari
    => non è possibile avere una complessità minore di O(n) perchè per assurdo
        se si riuscisse a non guardare tutti i bit, allora esiste un bit che non
        viene guardato. Quindi cambiando tale bit otterrei un risultato sbagliato.

Un problema ha complessità O(f(n)) se esiste un algoritmo che lo risolvei in tempo Ω(f(n)).
Un problema ha complessità Ω(f(n)) se tutti i possibili algoritmi che lo risolvono hanno
complessità Ω(f(n)).
Un problema ha complessità ϴ(f(n)) se trovo un algoritmo che lo risolva in tempo O(f(n))
e dimostro che non esistono algoritmi migliori per risolvere tale problema.


Es. Moltiplicazione di numeri binari
* Algoritmo classico: ϴ(n²)
* Algoritmo di Karatsuba: ϴ(n⅔)
    Supponendo che X e Y siano composti entrambi da n cifre.
    X = a*2^(n/2) + b
    Y = c*2*(n/2) + d
    => X * Y = ac * 2^n + (ad + bc) * 2^(n/2) + bd
    Calcolando così avrei T(n) = 4*T(n/2) + n che per il master theorem ha complessità O(n^2).
    => nessun vantaggio, costanti maggiori.
    * Ma posso utilizzare un trucco. Calcolo M1 = (a+b) * (c+d) = ac + ad + bc + bd con costo
        O(n) + T(n/2).
        Calcolo M2 = ac, M3 = bd.
        => Res = M2 * 2^n + (M1 - M2 - M3) * 2^(n/2) + M3.
        Complessità: T(n) = 3T(n/2) + ϴ(n) = n^(log3) = n^1.58

Es. Ordinamento
    Selection Sort: Caso pessimo, medio, ottimo ϴ(n^2)
    Insertion Sort: Caso pessimo(ordinato rev.) n^2, ottimo(ordinato) n, medio(torno indietro i/2 passi ogni ciclo) n^2
    Merge Sort: basato su divide et impera, divido il vettore in due sottovettori ognuno grande n/2.
                Ordino ricorsivamente le due metà, finchè ottengo un sottovettore di dimensione 1.
                Infine, ottenute due metà ordinate, posso unirle più velocemente in tempo O(n).
                Questo mi permette di ottenere un algoritmo di ordinamento che opera in tempo ϴ(nlogn) per ogni input.
                Ricorrenza [Assumendo n=2^k]: T(n) = 2*T(n/2) + n => ϴ(n*logn)

### ANALISI DI FUNZIONI ###
* Espressioni polinomiali => f(n) = ϴ(n^k) dove k è l'esponente maggiore presente nell'espressione.

@PROPRIETÀ delle funzioni di costo
* DUALITÀ: f(n) = O(g(n)) <=> g(n) = Ω(f(n))
    Infatti f(n) <= c*g(n)
            g(n) >= 1/c * f(n)
* ELIMINAZIONE DELLE COSTANTI
    f(n) = O(g(n)) <=> a*f(n) = O(g(n)), analogamnete con omega
    Infatti f(n) <= c*g(n) <==> a*f(n) <= a*c*g(n)

* SOMMATORIA (SEQUENZA DI ALGORITMI)
    f1 = O(g1), f2 = O(g2) => f1 + f2 = O(max(g1, g2))
    f1 = Ω(g1), f2 = Ω(g2) => f1 + f2 = Ω(min(g1, g2))

    Infatti f1 <=c1*g1, f2 <= c2*g2 ->
            f1 + f2 <= c1*g1 + c2*g2 <= (c1+c2) * max(g1, g2)

* PRODOTTO
    f1 = O(g1), f2 = O(g2) => f1 * f2 = O(g1 * g2)
    f1 = Ω(g1), f2 = Ω(g2) => f1 * f2 = Ω(g1 * g2)

* SIMMETRIA
    f = ϴ(g) <=> g = ϴ(f)

* TRANSITIVITÀ
    f = O(g), g = O(h) => f = O(h)

@ ANALISI PER LIVELLI o DELL'ALBERO DI RICORSIONE
"Srotolo" la ricorrenza in un albero i cui nodi rappresentano i costi ai vari livelli della ricorsione
[Srotolo la ricorrenza direttamente (diventa complesso) oppure disegno le chiamate ricorsive
ai vari livelli con la rispettiva dimensione dell'input, il costo della chiamata e il numero di chiamate, quindi
lo esprimo sotto forma di sommatoria]
@ ANALISI PER SOSTITUZIONE o PER TENTATIVI
Si cerca di "indovinare" una soluzione e si dimostra che è corretta tramite induzione.
@METODO DELL'ESPERTO
Master Theorem o teorema delle ricorrenze lineari con partizioni bilanciate
Siano a>=1, b>=2 ∈ ℕ, c>0, β>=0 ∈ ℝ
T(n) = {
    a*T(n/b) + c*n^β se n >1
    d                se n<= 1
}
definiamo α = log_b(a)
...
@ MASTER THEOREM ESTESO, PARTIZIONI BILANCIATE DI ORDINE COSTANTE


### ANALISI AMMORTIZZATA ###
Tecnica di analisi di complessità che considera il tempo richiesto per eseguire, nel CASO PESSIMO,
un'intera SEQUENZA di operazioni su una struttura dati.
L'idea è che esistono operazioni più o meno costose. Se le operazioni costose sono poco frequenti
allora il loro costo poù essere ammortizzato da quelle meno costose.

Metodi: Aggregazione, Accantonamenti e Potenziale
* Aggregazione - Si calcola il costo di n operazioni e si divide il costo per n
Es. contatore binario k bit [vettore di booleani] - n operazioni di incremento
Incremento: si nota che la cifra meno significativa viene cambiata ad ogni incremento,
la seconda ogni 2, ecc. Quindi il costo per effettuare n operazioni è dato da:
n + n/2 + n/4 + ... = sommatoria da 1 a k di n/2^i = n * sum 1/2^i che per k che tende all'infinito
è uguale a 2*n. Quindi ogni operazione ha costo ammortizzato 2, ossia costante.

* Accantonamenti - Alle operazioni vengono assegnati costi ammortizzati potenzialmente
distinti per le varie operazioni che possono essere maggiori/minori del loro costo effettivo.
Alle operazioni meno costose si assegna un costo maggiore di quello effettivo, aggiungendo un credito.
Tale credito sarà poi sottratto al costo effettivo delle operazioni più costose per ottenerne il costo
ammortizzato. Nota: la somma dei costi ammortizzati deve essere maggiore di quella dei costi
effettivi.
Es. contatore binario k bit [vettore di booleani] - n operazioni di incremento
Incremento: partendo da sinistra metto tutti gli 1 a 0 finchè trovo uno 0. Quindi
cambio quello 0 in 1. Si nota che ad ogni operazione trasformo un solo 0 in 1.
Quindi, se ogni volta che cambio uno 0 in 1 "pago" anche il costo per ricambiarlo in 0
in un incremento successivo, ad ogni operazione assegno costo ammortizzato 2.
[Nota che la somma è sempre maggiore di quella dei costi effettivi].

* Potenziale - Si associa alla struttura dati D una funzione di potenziale Φ(D),
  incrementata dalle operazioni meno costose e decrementata da quelle più costose.
  Il costo ammortizzato è dato da costo effettivo + la variazione di potenziale tra l'i-esima
  operazione e quella precedente. Perciò si dimostra che il costo ammortizzato di n operazioni è dato
  dal costo effettivo + la ddp tra l'n-esima operazione e la prima.
Es. Contatore binario. Il valore della funzione di potenziale è dato dal numero di bit 1 presenti nel
contatore.
Sia t il numero di bit 1 incontrati a partire da quello meno significativo. Poichè ad ogni incremento
tutti questi andranno a 0 e si aggiungerà un solo 1:
* il costo effettivo è t + 1
* la variazione di potenziale è 1 - t
* il costo ammortizzato è 2
All'inizio ho 0 bit a 1 => Φ(D_0) = 0, alla fine ne avrò >= 0, quindi la ddp è non negativa,
quindi il costo ammortizzato è una sovrastima di quello effettivo

+++ VETTORI DINAMICI +++
Si alloca un vettore di una certa dimensione detta capacità. L'inserimento in fondo costa O(1).
Problema: la capacità è costante, può essere insufficiente.
Soluzione: una volta riempito il vettore, se ne alloca uno più grande [ quanto? ] e si copia
il contenuto all'interno di quello nuovo.
--- INSERIMENTO ---
Come incrementare?
a) raddoppio           b) aggiungo incremento costante
 a) dimensione iniziale 1
  costo c_i = {
              i se i = 2^k +1
              1 altrimenti
            }
  T(n) = sum of c_i from 1 to n = sum of 1 from 1 to n + sum of 2^i from 0 to ⌊㏒(n)⌋
       <= n + sum of 2^i from 0 to ㏒(n) = n + 2^(log(n) + 1) -1 = n + 2n -1 = ϴ(n)
  b) dimensione iniziale d, incremento d
  c_i = {
    i se i mod d = 1
    1 altrimenti
  }
  T(n) = n + sum d*j from 1 to ⌊n/d⌋ <= n + d * (n/d)*(n/d +1)/2 =
       = n + n*(n/d + 1)/2 = ϴ(n²)
  => molto peggiore, ogni operazione costa n !

--- CANCELLAZIONE ---
Quando decrementare la dimensione?
Fattore di carico α = dim/capacità
-> se contraggo guando α = 1/2, se inserisco subito devo raddoppiare... non una buona idea
-> Se dimezzo quando α = 1/4, dimezzando diventerà 1/2.

Usiamo una funzione di potenziale che:
- vale 0 all'inizio e subito dopo una espansione/contrazione
- cresce fino a raggiungere il numero di elementi presenti nella tavola quando α
  aumenta fino a 1 o diminuisce fino a 1/4.

  Φ = {
        2*dim - capacità se α >= 1/2
        capacità/2 - dim se α <= 1/2
      }
  Nota: α = 1/2 => phi = 0              [dopo espansione/contrazione]
        α =   1 => phi = capacità = dim [ prima di espansione ]
        α = 1/4 => phi = dim            [prima di contrazione]

    Prima di contrazione/espansione il potenziale è sufficiente per pagare il loro costo.
