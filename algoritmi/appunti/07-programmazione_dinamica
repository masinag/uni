### PROGRAMMAZIONE DINAMICA
Basato su divide et impera, utilizzato quando lo stesso sotto-problema deve essere risolto molte volte.
In questo caso la soluzione viene memorizzata in una tabella indicizzabile in O(1) in modo da
risolvere ogni sotto-problema una volta sola.

*Problema: DOMINO
  Tessere grandi 2x1. Dato un intero n, restituire il numero di disposizioni di n tessere in un rettangolo
  2xn.
  Se non ho tessere -> 1
  Se ho 1 tessera -> 1
  Altrimenti, posso mettere l'n-esima tessera in verticale, dovendo disporre le n-1 precedenti,
  oppure in orizzontale, dovendo mettere la n-1-esima in orizzontale e quindi dovendo disporre le
  n-2 precedenti => DP[n] = DP[n-1] + DP[n-1] se n > 1
                            1 altrimenti
  => Fibonacci!
  * Scritto cos√¨ ha complessit√† esponenziale [ Teorema delle ricorrenze lineari di ordine costante,
    a = 2 => Œò(2^n)] perch√® molti sotto-problemi sono ripetuti!
  * Soluzione: Memorizziamo ogni risultato calcolato in una tabella DP che contiene un elemento per
               ogni sottoproblema.
               L'iterazione pu√≤ essere top-down(Memoization) o bottom-up.
               Complessit√† Œò(n) sia in tempo che in spazio
  * Osservo che utilizzo sempre solo gli ultimi 2 risultati calcolati => posso salvare solo quelli
    utilizzando œ¥(1) spazio.
  # Nota che la serie cresce molto rapidamente (esponenzialmente) => sono richiesti œ¥(n) bit per
    memorizzare l'n-esimo numero di fibonacci => tutte le Complessit√† devono essere moltiplicate per
    un fattore n. œ¥(n¬≤) tempo, Œò(n) spazio

* Problema: HATEVILLE
  Villaggio composto da n case, numerate da 1 a n lungo una singola strada. Ognuno odia i propri vicini
  da entrambi i lati (i-1, i+1). Hateville vuole organizzare una sagra e abbiamo il compito di raccogliere
  i fondi massimizzandone il valore, sapendo che i √® disposto a donare D[i] ma se e solo se entrambi i
  propri vicini non donano.
  Definiamo HV(i) uno dei possibili insiemi di indici da selezionare per ottenere una donazione ottimale
  dalle prime i case. HV(n) √® la soluz. al problema originale.
  HV(i) = highest(i ‚äï HV(i-2), HV(i-1)) (prendo l'i-esimo elemento oppure no)

  Dimostrazione correttezza- sottostruttura ottima
  Sia HV_p(i) il problema dato alle prime i case,
      HV_s(i) una soluzione ottimale per HV_p(i).
  Segue che:
    1. se i ‚àâ HV_s(i), allora HV_s(i) = HV_s(i-1)
    2. se i ‚àà HV_s(i), allora HV_s(i) = HV_s(i-2) ‚à™ {i}
  DIMOSTRAZIONE:
    1. Supp che HV_s(i) non sia una soluzione ottima per HV_p(i-1), allora esisterebbe una
       soluzione migliore per i-1, ma che sarebbe a sua volta soluzione del problema i, migliore
       di quella ottima, ma ci√≤ √® assurdo.
    2. Sappiamo che i-1 ‚àâ HV_s(i), quindi togliendo i otteniamo una soluzione ottima per i-2.
        Se cos√¨ non fosse, esisterebbe una soluzione migliore per i-2, che concatenata ad i
        produrrebbe una soluzione migliore di quella ottima per il problema i, assurdo.

        Nella tabella, memorizzare gli insiemi sarebbe costoso, per questo si memorizza solo la somma
        da cui sar√† infine possibile risalire all'insieme di indici ottimo
  DP[i] = {
      max(D[i] + DP[i-2], DP[i-1]) se i>=2 # scelgo se prendere l'i-esimo oppure no
      D[i] se                      se i==1
      0                            se i==0
  }

  Data la tabella, possiamo risalire alla soluzione ottima partendo da DP[n] vedendo se √® uguale a
  DP[n-1] (non ho preso n, vado a n-1) oppure no (ho preso n, vado a n-2). Se non voglio risalire alla
  soluzione, posso memorizzare solo gli ultimi 2 risultati calcolati riducendo la complessit√† spaziale.

* Problema: Zaino
  Dato un insieme di oggetti, ognuno caratterizzato da un peso w e da un profitto p, e uno zaino con un
  limite di capacit√† C, individuare un sottoinsieme di oggetti il cui peso sia inferiore alla capacit√†
  dello zaino e il cui valore sia massimale (il pi√π alto tra tutti gli altri sottoinsiemi validi di
  oggetti).

  Definiamo DP[i][c] come il massimo profitto ottenibile dai primi i<=n oggetti con uno zaino di
  capacit√† c. Il problema originale √® rappresentato da DP[n][C]

  DP[i][c] = max(DP[i-1][c], p[i] + DP[i-1][c-w[i]]) se i >= 1 ‚àß c > 0 # non lo prendo vs lo prendo
             0                                       se i == 0 ‚à® c == 0
             -‚àû                                      se c < 0
  Posso ricostruire la soluzione partendo da DP[n][C] e verificando da dove provengo

  # ATTENZIONE: T(n) = O (n*C) dove C √® rappresentato con k bit (k = logC) => l'algoritmo √®
    pseudo polinomiale T(n) = O(n*2^k)

  * Soluzione con memoization.
    Visto che la complessit√† √® pseudo-polinomiale, conviene usare la versione ricorsiva? No perch√©
    T(n) = 2T(n-1) +1 = O(2^n) [ conviene se 2^n << n*C ]
    Osserviamo per√≤ che non tutti gli elementi della matrice sono utili, la maggior parte vengono
    calcolati inutilmente => posso utilizzare la versione ricorsiva evitando di ricalcolare i sotto-problemi
    gi√† risolti (MEMOIZATION). In realt√† non ho un vantaggio perch√© la tabella deve essere comunque
    inizializzata. Posso invece utilizzare una hash table ottenendo un costo pari a O(min(2^n, n*C))

* Problema: Zaino senza limiti di scelta: un oggetto pu√≤ essere preso pi√π volte rispettando i vincoli
            sulla capacit√†, massimizzando il profitto

            DP[i][c] = { max(DP[i-1][c], p[i] + DP[i][c-w[i]]) se i >= 1 e c > 0
                          0   se c == 0 or i == 0
                          - ‚àû se c < 0
                        }
            L'algoritmo converge comunque perch√© o l'indice o la capacit√† diminuiscono sempre
            # Si pu√≤ tuttavia semplificare la formula riducendo lo spazio occupato œ¥(C), Complessit√†
              sempre O(nC)[non √® detto che le debba riempire tutte] perch√© per ogni casella devo
              scorrere tutti gli n oggetti.
            DP[c] = {
                0 se c==0
                max_w[i]<=c {DP[c-w[i] + p[i]]}
            }

            # Questo approccio rende pi√π difficile ricostruire la soluzione, conviene memorizzare l'indice
            # da cui deriva il massimo

* Problema: LCS
    Date due stringhe, trovare la pi√π lunga sottosequenza comune.
    Applicazioni: Bioinformatica, confronto tra sequenze di DNA
                  diff per verificare le modifiche fatte a un file ( a livello di riga)

    @ Una sequenza P √® una SOTTOSEQUENZA di T se P √® ottenuto da T rimuovendo uno o pi√π dei suoi elementi.
      I rimanenti elementi sono elencati nello stesso ordine , senza essere necessariamente contigui.
    @ Una sequenza X √® una SOTTOSEQUENZA COMUNE di due sequenze T, U se √® sottosequenza sia di T che di U
      X ‚àà CS(T, U)
    @ Una sequenza X ‚àà CS(T, U) √® una SOTTOSEQUENZA COMUNE MASSIMALE di T e U se non esiste Y ‚àà CS(T, U)
      tale che |Y| > |X|
      X ‚àà LCS(T,U)
    -> Trovare una LCS tra T e U
    * Brute force -> per ogni sottosequenza di T controllo se √® anche di U ->nr. di sottoseq √® 2^n
      => Complessit√† ùõâ(2^n(n+m)) [verficare se √® sottoseq. di U]

    @ Data una sequenza T, il prefisso T(i) √® una sequenza data dai primi i caratteri di T

    Possiamo scrivere la soluzione in maniera ricorsiva
    # se gli ultimi caratteri sono uguali, li considero e riduco il problema ai caratteri precedenti
    # se gli ultimi caratteri sono differenti, scelgo la LCS massima tra pi√π le due soluzioni togliendo
      l'ultimo carattere di T oppure l'ultimo di U
    LCS(T(i), U(j)) = {
        LCS(T(i-1), U(j-1)) ‚äï t_i se  i>=1, j>=1, t_i == u_j
        longest(LCS(T(i-1), U(j)), LCS(T(i), U(j-1))) se i>=1, j>=1, t_i != u_j
        ‚àÖ se i==0 or j == 0
    }
    Dimostrazione correttezza per assurdo
    Teorema sottostruttura ottima:
    Date due sequenze T=(t1,..., tn), U=(u1,...um), sia X = (x1,..., xk) una LCS di T e U. Allora:
    1. se tn == um => xk = tn = um e X(k-1) ‚àà LCS(T(n-1), U(m-1))
    2. se tn != um  xk != tn => X ‚àà LCS(T(n-1), U)
    3. se tn != um  xk != um => X ‚àà LCS(T, U(m-1))

    Dimostrazione:
    1. supponiamo xk != tn = um. Allora (X ‚äï xk) ‚àà CS(T, U) e |Y| > |X|, assurdo
       supponiamo che X(k-1) ‚àâ LCS(T(n-1), U(m-1)) >= ‚àÉ Y ‚àà LCS(T(n-1), U(m-1)) tale che
                  |Y| > |X| ma allora Y ‚äï tn ‚àà CS(T, U) ed √® pi√π lunga di X, assurdo
    2. supponiamo X ‚àâ LCS(T(n-1), U) => Esiste Y ‚àà LCS(T(n-1), U) tale che |Y| > |X|, ma allora
                  Y ‚àâ CS(T,U) ed √® pi√π lunga di X, assurdo
    3. simmetrico

    Per calcolare la lunghezza:

    DP[i][j] = {
        0    i=0 or j=0
        1+ DP[i-1][j-1] i>0, j>0, t_i = u_j
        max(DP[i-1][j], DP[i][j-1]) i>0, j>0, t_i != u_j
    }
    DP[n][m] √® la lunghezza della soluzione, per ricostruire la LCS √® sufficiente paritire da
    DP[n][m] e scorrerla al contrario per risalire le scelte ottimali fatte.
    Se voglio calcolare solo la lunghezza √® sufficiente memorizzare solo due righe

    Complessit√†: data dal riempire la tabelle O(mn), per ricostruire la soluzione O(m+n)

* Problema: STRING MATCHING APPROSSIMATO
            Dati una stringa P (pattern) e una stringa T (testo). Un'occorrenza k-approssimata di
            P in T √® una copia di P in T in cui sono ammessi k "errori" tra caratteri, del tipo:
            - sostituzione (i caratteri sono diversi)
            - inserimento (un carattere di P non √® incluso in T)
            - cancellazione (un carattere di T non √® incluso in P)
            => trovare un'occorrenza k-approssimata per cui k sia minimo
      Sia DP[0,...,m][0...n] una tabella di programmazione dinamica tale che DP[i][j] contiene il minimo
      valore k per cui esiste un'occorrenza k-approssimata di P(i) in T(j) che termina nella posizione j.
      DP[i][j] ={
          0                               se i = 0
          i                               se j = 0
          min(DP[i-1][j-1] + iif(pi = tj, 0, 1), sostituzione o avanzamento su entrambi
              DP[i-1][j],   inserimento
              DP[i][j-1]     cancellazione se pi != tj
          )
     }
     # La soluzione √® data dal pi√π piccolo valore DP[m][j] per 0<=j<=m
* Problema: PRODOTTO CATENA DI MATRICI
    Data una sequenza di n matrici compatibili due a due al prodotto, vogliamo calcolare il loro prodotto.
    Il prodotto di matrici non √® commutativo ma √® associativo.
    Poich√© si basa sul prodotto tra scalari, vogliamo minimizzare il numero di moltiplicazioni scalari
    per calcolare il prodotto. Infatti vale la pena preprocessare i dati per cercare la parentesizzazione
    migliore per risparmiare tempo dopo nel calcolo vero e proprio

    @ Una PARENTESIZZAZIONE P_i,j del prodotto Ai‚ãÖA_i+1‚ãÖ...‚ãÖAj consiste:
      * nella matrice Ai se i = j
      * nel prodotto di due parentesizzazioni (P_i,k ‚ãÖ Pk+1,j) se i < j
      In questo caso il prodotto evidenziato √® detto ultimo PRODOTTO.
    @ una PARENTESIZZAZIONE OTTIMA √® la parentesizzazione che richiede il minor numero di moltiplicazioni
      scalari per essere completata, fra tutte le parentesizzazioni possibili.
      Quante sono le parentesizzazioni possibili P(n)? L'ultimo prodotto pu√≤ occorrere in n-1 posizioni
      diverse. Fissato l'indice k dell'ultimo prodotto, abbiamo P(k) parentesizzazioni per A1‚ãÖ...‚ãÖAk,
      P(n-k) per A_k+1‚ãÖ...‚ãÖAn
      P(n) = {
        1 se n = 1
        n-1
         ‚àë  P(k)*P(n-k)           Cresce come un esponenziale (numero di Catalan)!
        k=1
      }
    Definiamo c_i-1 il numero di righe della matrice Ai
             c_i il numero di colonne di Ai
             A[i...j] il sottoprodotto Ai‚ãÖ...‚ãÖAj
             P[i...j] una parentesizzazione per A[i...j] (anche non ottima)
    # Teorema: data una parentesizzazione ottima P[i...j] di A[i...j], esiste un
      ultimo prodotto a un indice k. I due sottoprodotti P[i...k] e P[k+1...j] sono
      a loro volta ottime per i prodotti A[i...k] e A[k+1...j]

      Dimostrazione:
      Supponiamo che esista P'[i...k] con costo inferiore a P[i...k]. Allora P'[i...k]‚ãÖP[k+1...j]
      sarebbe una parentesizzazione di A[i...j] con costo inferiore a P[i...j], assurdo
    # Sia DP[i][j] il numero minimo di moltiplicazioni scalari necessarie per calcolare il prodotto A[i...j]
      * se i = j DP[i][j] = 0
      * altrimenti
          DP[i][j] = min_i<=k<j(DP[i][k] + DP[k+1][j] + ci-1*ck*cj) [ci-1*ck*cj √® il costo dell'ultimo
          prodotto].
          Salvo in una matrice anche la posizione dell'ultimo prodotto della soluzione ottima in modo
          da ricostruirla in O(n)
    # Nota: √® necessario riempire solo la diagonale superiore della matrice (i<=j)
            Il valore di una cella dipende da valori che le stanno "sotto" e a "sinistra"
            => Devo scorrere la matrice per diagonali, dalla diagonale principale "in su".
    # Il risultato √® DP[1][n]
    # Complessit√† O(n^3) (O(n^2) celle da riempire, ognuna richiede O(n))

* Problema: INSIEME INDIPENDENTE DI INTERVALLI PESATI
    Dati n intervalli distinti aperti a destra della retta reale, dove all'intervalli i √® associato
    un profitto wi 1<=i<=n.
    @ due intervalli i, j si dicono DISGIUNTI se bj<=ai oppure bi <= aj
    Obiettivo: trovare un insieme indipendente di peso massimo, ovvero un sottoinsieme di intervalli
    disgiunti tra loro tale che la somma dei loro profitti sia massima.

    Per usare la programmazione dinamica √® necessario preelaborare i dati ordinando gli intervalli
    per tempo di fine crescente.
      DP[i] rappresenta il profitto massimo ottenibile con i primi i intervalli
      DP[i] = {
        0                       se i = 0
        max(DP[i-1], wi + max{DP[j] | j<i, bj <= ai}) se i > 0
      }
      complessit√† O(n^2)

    # possiamo effettuare un'altra preelaborazione, pre-calcolando il predecessore j di i
      dove i<j √® il massimo indice tale che bj <= ai, se non esiste √® 0

      DP[i] = {
        0 se i = 0
        max(DP[i-1], wi + DP[pred[i]]) se i > 0
      }
      Costo O(n)
      Come calcolare i precedenti? Soluzione banale O(n^2)
        -> posso utilizzare la ricerca binaria O(nlog(n)) [uguale all'ordinamento]
      => costo algoritmo O(nlogn);
